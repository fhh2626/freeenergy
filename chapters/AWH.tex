% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
\section{Accelerated Weight Histogram\label{Sec:ES:AWH}}
The accelerated weight histogram (AWH) method is one of the extended ensemble or generalized ensemble methods, and was proposed by Lidmar in 2012\cite{LidmarPRE2012} and extended by Lidmar and Hess later\cite{LindahlJCP2014,LundborgJCP2021}. In generalized ensemble methods, the parameters are tuned to ensure each state, corresponding a certain set of values of the parameters, is populated equally in an average sense. However, the best estimate of these parameters are usually unknown before hand. Similar to the Wang-Landau's idea, the accelerated weight histogram method designs an iterative way to update the parameters until a desired distribution of the extended ensemble has been reached.

Consider a model described by a probability distribution $\pi_\lambda(x)$, which depends on parameters $\lambda$, and $x$ denotes the configuration of the system. The parameter $\lambda$ can be either a scalar or a vector, and it can take a discrete set of preselected values $\lambda_m\in \mathcal{M}=\{\lambda_1,\lambda_2,\dots,\lambda_M\}$. In an extended ensemble simulation, states are sampled according to a joint distribution $P(x,\lambda)$, which is expressed as
\begin{equation}
	P(x,m)=\frac{1}{\mathcal{Z}}e^{f_m-E_m(x)}.
\end{equation} 
The weights $e^{f_m}$ allow tuning the marginal distribution $P(m)$ to approach any desired form. For any fixed $\lambda_m$, we can generate samples, using molecular dynamics or Markov chain Monte Carlo, from the conditional distribution
\begin{equation}
	P(x|m)\equiv \pi_m(x)=e^{F_m-E_m(x)}.
\end{equation}
Usually, this can be done without knowledge of $F_m$. 

Complementing the ordinary (MD or MC) moves, transitions in parameter space have to be carried out to make the marginal distribution
\begin{equation}
	P(m)=\sum_x P(x,m)=\frac{1}{\mathcal{Z}}e^{f_m-F_m}
\end{equation}
approximately flat. It indicates that $f_m\approx F_m$. where $F_m$ is the exact (dimensionless) free energy at $\lambda_m$. However, it is unknown at the beginning of the simulation.

In the AWH algorithm, $f_m$ is updated in an iterative way. The whole procedure can be summarized as follows:
\begin{enumerate}[label=(\arabic*)]
	\item Perform $N_x$ updates of the configurations $x$ at fixed parameter value $\lambda_m$,
	\item Perform a parameter move $m \to m^\prime$ using the Gibbs sampler using
	\begin{equation}
		w_{m^\prime m}(x)=P(m^\prime|x)=\frac{e^{-E_{m^\prime}(x)-f_{m^\prime}}}{\sum\limits_{k\in \mathcal{M}}e^{E_k(x)-f_k}},
	\end{equation}
    which is independent of $m$. Therefore, the subscript $m$ can be omitted, and we denote it as $w_{m^\prime}$. 
    \item Update the weight histogram using
    \begin{equation}
        W_m\leftarrow W_m+w_{m}(x),\quad \forall m. 
    \end{equation}
    Sample any observables of interest using
    \begin{equation}
    	\langle A\rangle_m=\frac{\sum_t A(x_t,m)w_{m}(x_t)}{\sum_t w_{m}(x_t)},
    \end{equation}
    where $\{x_t\}$ denote the time series of visited configurations.
    \item Repeat steps 1-3 until $N_I$ samples have been obtained.
    \item Update the free energy parameters $f_m$ using
    \begin{equation}
    	f_m\leftarrow f_m-\ln\left(\frac{W_m\mathcal{M}}{N}\right) \quad \forall m
    \end{equation}
    and the weight histogram using
    \begin{equation}
    	W_k\leftarrow N/\mathcal{M}.
    \end{equation}
    \item Start a new iteration from step 1 unless the desired accuracy has been reached.
\end{enumerate}
